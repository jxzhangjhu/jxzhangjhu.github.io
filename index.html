<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Jiaxin Zhang</title> <meta name="author" content="Jiaxin Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jxzhangjhu.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <article> <div class="row"> <div class="col-sm-3"> <img style="width: 190px;border-radius: 190px" src="/assets/img/prof_pic.jpg"> </div> <div class="col-sm-9"> <h1 class="post-title"> <span class="font-weight-bold">Jiaxin</span> Zhang </h1> <p class="desc"></p> <p>AI Researcher<br>Salesforce AI Research</p> <div class="social"> <div class="contact-icons"> <a href="mailto:%6A%78%7A%68%61%6E%67%61%69@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-7576-6110" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=LiDm8jEAAAAJ&amp;hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/jxzhangjhu" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/jiaxin-zhang-1425289b" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/jxzhangjhu" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> </div> </div> </div> <div class="clearfix"> <p>I am a Research Scientist at <a href="https://www.salesforceairesearch.com/" rel="external nofollow noopener" target="_blank">Salesforce AI Research</a>, specializing in Reliable AI Agents and Foundation Model.</p> <p><strong>Previously</strong>: I was a Senior Staff Research Scientist and founding member, building AI Research Team at <a href="https://www.intuit.com/" rel="external nofollow noopener" target="_blank">Intuit</a>. Prior to this, I was a Staff Research Scientist in Computer Science and Mathematics Division at <a href="https://www.ornl.gov/" rel="external nofollow noopener" target="_blank">Oak Ridge National Laboratory (ORNL)</a>. I received my Ph.D. from the <a href="https://www.jhu.edu/" rel="external nofollow noopener" target="_blank">Johns Hopkins University</a>.</p> <p></p> <p>üí¨ I‚Äôm always looking for highly motivated Ph.D. students to work with me for research internship positions. Please feel free to email me with your CV if interested.</p> </div> <h2 style="font-size: 1.5rem; margin-top: 2.5rem; margin-bottom: 0.5rem; font-weight: bold;"><a href="/news/" style="color: inherit;">News &amp; Updates</a></h2> <hr style="border: none; border-top: 2px solid #ddd; margin-top: 0; margin-bottom: 1rem;"> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless" id="news-table" style="margin-bottom: 0;"> <tr class="news-item-visible" style="line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">10/2024</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> [Invited Talk] I will give a talk in NeurIPS 2024 Workshop <a href="https://interpretable-ai-workshop.github.io/" rel="external nofollow noopener" target="_blank">‚ÄúInterpretable AI: Past, Present and Future‚Äù</a>, Dec, 2024, Vancouver, Canada! </td> </tr> <tr class="news-item-visible" style="line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">10/2024</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> [EMNLP x 6] Six Long Papers (3 Main, 1 Findings, 2 Industry Track) are accepted by <a href="https://2024.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2024</a>. 2 oral presentations and 4 poster presentations! See you in Miami! </td> </tr> <tr class="news-item-visible" style="line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">06/2024</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> [Invited talk] I will present my research on hallucination detection and mitigation at <a href="https://www.linkedin.com/showcase/intuitoss/posts/" rel="external nofollow noopener" target="_blank">Intuit Open Source Meetup</a>! </td> </tr> <tr class="news-item-visible" style="line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">05/2024</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> One UQ paper was accepted by <a href="https://aistats.org/aistats2024/" rel="external nofollow noopener" target="_blank">AISTATS 2024</a>. See you in Valencia, Spain! </td> </tr> <tr class="news-item-visible" style="line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">03/2024</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> One paper on UQ for LLM was accepted by <a href="https://2024.eacl.org/" rel="external nofollow noopener" target="_blank">EACL 2024</a>. </td> </tr> <tr class="news-item-visible" style="line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">11/2023</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> I created two Github Repos to share resources and papers on <a href="https://github.com/jxzhangjhu/Awesome-LLM-Prompt-Optimization" rel="external nofollow noopener" target="_blank">LLM Prompt Optimization</a> and <a href="https://github.com/jxzhangjhu/Awesome-LLM-RAG" rel="external nofollow noopener" target="_blank">LLM RAG</a>. Welcome to contribute and work together! </td> </tr> <tr class="news-item-visible" style="line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">10/2023</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> Two papers on ‚ÄúDECDM: Document Enhancement using Cycle-Consistent Diffusion Models‚Äù and ‚ÄúOn the Quantification of Image Reconstruction Uncertainty without Training Data‚Äù are accpeted by WACV 2024! </td> </tr> <tr class="news-item-visible" style="line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">10/2023</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> Our paper on ‚Äú<a href="https://openreview.net/forum?id=WcGXAxhC81" rel="external nofollow noopener" target="_blank">A Divide-Conquer-Reasoning Approach to Consistency Evaluation and Improvement in Blackbox Large Language Models</a>‚Äù is accepted by NeurIPS 2023 Workshop on Socially Responsible Language Modelling Research. </td> </tr> <tr class="news-item-visible" style="line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">10/2023</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> Our paper on <a href="https://arxiv.org/abs/2311.01740" rel="external nofollow noopener" target="_blank">SAC^3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency</a> is accepted by EMNLP 2023! The <a href="https://github.com/intuit/sac3" rel="external nofollow noopener" target="_blank">code</a> is coming soon! </td> </tr> <tr class="news-item-visible" style="line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">09/2023</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> One patent on ‚Äú<a href="https://patents.google.com/patent/US11769239B1/en" rel="external nofollow noopener" target="_blank">Model based document image enhancement</a>‚Äù is issued and published. </td> </tr> <tr class="news-item-hidden" style="display: none; line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">09/2023</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> Our paper on ‚Äú<a href="https://arxiv.org/abs/2310.20153" rel="external nofollow noopener" target="_blank">Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision</a>‚Äù is accepted by NeurIPS 2023! Cheers! </td> </tr> <tr class="news-item-hidden" style="display: none; line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">03/2023</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> I built a <a href="https://github.com/jxzhangjhu/Awesome-LLM-Reliability-Robustness-Safety" rel="external nofollow noopener" target="_blank">Github Repo</a> that contains a collection of resources and papers on Reliability, Robustness and Safety in Large Language Models (LLMs). </td> </tr> <tr class="news-item-hidden" style="display: none; line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">02/2023</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> Our paper titled ‚Äú<a href="https://arxiv.org/abs/2302.10441" rel="external nofollow noopener" target="_blank">Speech Privacy Leakage from Shared Gradients in Distributed Learning</a>‚Äù is accepted by ICASSP 2023! </td> </tr> <tr class="news-item-hidden" style="display: none; line-height: 1.4;"> <th scope="row" style="white-space: nowrap; padding-right: 15px; vertical-align: top; width: 80px;">12/2022</th> <td style="padding-top: 0.3rem; padding-bottom: 0.3rem;"> Two papers on ‚Äú<a href="https://arxiv.org/abs/2212.01016" rel="external nofollow noopener" target="_blank">Accelerating Inverse Learning via Intelligent Localization with Exploratory Sampling</a>‚Äù and ‚Äú<a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LiDm8jEAAAAJ&amp;sortby=pubdate&amp;citation_for_view=LiDm8jEAAAAJ:OcBU2YAGkTUC" rel="external nofollow noopener" target="_blank">AutoNF: Automated Architecture Optimization of Normalizing Flows with Unconstrained Continuous Relaxation Admitting Optimal Discrete Solution</a>‚Äù are accpeted by AAAI 2023! </td> </tr> <tr> <td colspan="2" style="text-align: center; padding-top: 15px;"> <button id="show-more-news" class="btn btn-sm btn-outline-primary" onclick="toggleNews()">Show all (4 more)</button> <button id="show-less-news" class="btn btn-sm btn-outline-primary" onclick="toggleNews()" style="display: none;">Show less</button> </td> </tr> </table> </div> <script>function toggleNews(){var e=document.querySelectorAll(".news-item-hidden"),l=document.getElementById("show-more-news"),n=document.getElementById("show-less-news");0!==e.length&&("none"===e[0].style.display||""===e[0].style.display?(e.forEach(function(e){e.style.display=""}),l&&(l.style.display="none"),n&&(n.style.display="")):(e.forEach(function(e){e.style.display="none"}),l&&(l.style.display=""),n&&(n.style.display="none")))}</script> </div> <h2 style="font-size: 1.5rem; margin-top: 2.5rem; margin-bottom: 0.5rem; font-weight: bold;">Recent Talks</h2> <hr style="border: none; border-top: 2px solid #ddd; margin-top: 0; margin-bottom: 1rem;"> <ul style="list-style-type: none; padding-left: 0;"> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>Dec 2024</strong>, NeurIPS @ Vancouver üá®üá¶</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>Nov 2024</strong>, EMNLP @ Miami üá∫üá∏</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>Jul 2024</strong>, ICML @ Vienna üá¶üáπ</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>May 2024</strong>, AISTATS @ Valencia üá™üá∏</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>Jan 2024</strong>, WACV @ Hawaii üá∫üá∏</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>Dec 2023</strong>, NeurIPS @ New Orleans üá∫üá∏</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>Dec 2023</strong>, EMNLP @ Singapore üá∏üá¨</li> </ul> <h2 style="font-size: 1.5rem; margin-top: 2.5rem; margin-bottom: 0.5rem; font-weight: bold;">Awards &amp; Honors</h2> <hr style="border: none; border-top: 2px solid #ddd; margin-top: 0; margin-bottom: 1rem;"> <ul style="list-style-type: none; padding-left: 0;"> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>CTO Award</strong>, Salesforce, 2024</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>A2D Innovation Award</strong>, Salesforce, 2024</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>Promising Early‚ÄëCareer Researcher Award</strong>, Oak Ridge National Laboratory, 2020</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>NeurIPS Travel Award</strong>, 2019</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>Acheson J. Duncan Graduate Research Award</strong>, Johns Hopkins University, 2018</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>Dean's Fellowship</strong>, Johns Hopkins University, 2014</li> <li style="margin-bottom: 0.5rem;">‚Ä¢ <strong>National Scholarship of P.R. China</strong>, 2009, 2012</li> </ul> <h2 style="font-size: 1.5rem; margin-top: 2.5rem; margin-bottom: 0.5rem; font-weight: bold;"><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> <hr style="border: none; border-top: 2px solid #ddd; margin-top: 0; margin-bottom: 1rem;"> <p style="margin-top: 0; margin-bottom: 1rem; font-size: 0.9rem;">(Full publication list can be found on <a href="https://scholar.google.com/citations?user=LiDm8jEAAAAJ&amp;hl=en" target="_blank" rel="external nofollow noopener">Google Scholar</a>)</p> <div class="publications"> <ol class="bibliography"> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div> <div id="chen2025nudging" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2509.25666" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Nudging the Boundaries of LLM Reasoning</a></div> <div class="author"> Justin Chih-Yao Chen,¬†Becky Xiangyu Peng,¬†Prafulla Kumar Choubey,¬†Kung-Hsiang Huang,¬†<em>Jiaxin Zhang</em>,¬†Mohit Bansal,¬†and¬†Chien-Sheng Wu</div> <div class="periodical"> <em></em> </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2509.25666" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2509.25666.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/SalesforceAIResearch/NuRL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reasoning</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have shown remarkable capabilities in reasoning tasks, yet they often struggle with complex multi-step problems. In this work, we propose NuRL (Nudging the Boundaries of LLM Reasoning), a novel approach that enhances LLM reasoning through strategic hint injection and self-improvement. Our method introduces a two-stage training framework: first, we use Group Relative Policy Optimization (GRPO) to improve base reasoning capabilities, then we apply self-generated or GPT-generated hints to guide reasoning trajectories. NuRL dynamically injects hints for samples where all rollouts fail, enabling the model to learn from its mistakes and improve reasoning quality. We evaluate our approach on multiple benchmarks including MATH-500, MATH-Hard, AIME24, MMLU-Pro, GPQA, and Date, demonstrating significant improvements in reasoning accuracy across different model sizes and architectures.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div> <div id="qiu2025mmpersuade" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2510.22768" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion</a></div> <div class="author"> Haoyi Qiu,¬†Yilun Zhou,¬†Pranav Narayanan Venkit,¬†Kung-Hsiang Huang,¬†<em>Jiaxin Zhang</em>,¬†Nanyun Peng,¬†and¬†Chien-Sheng Wu</div> <div class="periodical"> <em></em> </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2510.22768" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2510.22768.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Multimodality &amp; Vision</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>As large vision-language models (LVLMs) are increasingly deployed in domains like shopping, health, and news, they encounter substantial persuasive content. A critical question is how and why these models, as persuadees, are influenced by multimodal inputs. Understanding their susceptibility to persuasion and the effectiveness of different persuasive strategies is crucial, as overly persuadable models may adopt misleading beliefs, ignore user preferences, or generate unethical or unsafe outputs when exposed to manipulative information. We introduce MMPersuade, a unified framework for systematically studying multimodal persuasion dynamics in LVLMs. MMPersuade provides (i) a comprehensive multimodal dataset pairing images and videos with established persuasion principles, covering commercial, subjective, behavioral, and adversarial scenarios; and (ii) an evaluation framework that quantifies persuasion effectiveness and model susceptibility through third-party consistency scores and self-estimated token probabilities from conversation history. Our study of six leading LVLMs as persuadees yields three key insights: (i) multimodal inputs significantly increase persuasion effectiveness and model susceptibility compared to text-only, especially in misinformation scenarios; (ii) stated prior preferences reduce susceptibility, but multimodal information maintains its persuasive advantage; and (iii) different strategies vary in effectiveness across scenarios, with reciprocity most effective in commercial and subjective scenarios, while credibility and logic dominate in adversarial scenarios. By jointly analyzing persuasion effectiveness and susceptibility, MMPersuade provides a principled foundation for developing models that are robust, consistent, and ethically aligned when handling persuasive multimodal content.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div> <div id="shen2025latteflow" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2506.06952" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer</a></div> <div class="author"> Ying Shen,¬†Zhiyang Xu,¬†Jiuhai Chen,¬†Shizhe Diao,¬†<em>Jiaxin Zhang</em>,¬†Yuguang Yao,¬†Joy Rimchala,¬†Ismini Lourentzou,¬†and¬†Lifu Huang</div> <div class="periodical"> <em></em> </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2506.06952" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2506.06952.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Multimodality &amp; Vision</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Recent advances in multimodal foundation models have unified image understanding and generation, opening exciting pathways for handling a wide range of vision-language tasks within a single framework. Despite progress, existing unified models typically require extensive pretraining and struggle to match the performance of models specialized for each task. Additionally, many of these models exhibit slow image generation speeds, limiting their practical applications in real-time or resource-constrained environments. In this work, we propose LaTtE-Flow, a novel and efficient architecture that unifies image understanding and generation in a single multimodal model. LaTtE-Flow builds upon strong pretrained vision-language models (VLMs), inheriting powerful multimodal understanding capabilities, and extends these capabilities through a novel layerwise timestep-expert flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow matching process across specialized transformer layer groups, with each group responsible for different timestep subsets. This design significantly improves sampling efficiency by activating only a small number of layers at each sampling timestep. To further enhance performance, we propose a timestep-conditioned residual attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow excels in multimodal understanding tasks while maintaining competitive image generation quality, with inference speed approximately 6x faster than recent unified multimodal models.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"> <abbr class="badge">EMNLP 2025</abbr><div style="margin-top: 0.25rem;"> <span style="display: inline-block; background-color: transparent; color: var(--global-theme-color); border: 1px solid var(--global-theme-color); padding: 0.15rem 0.5rem; border-radius: 8px; font-size: 0.7rem; font-weight: 500;">Oral</span> </div> </div> <div id="chen2025r2ibench" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2505.23493" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation</a></div> <div class="author"> Kaijie Chen,¬†Zihao Lin,¬†Zhiyang Xu,¬†Ying Shen,¬†Yuguang Yao,¬†Joy Rimchala,¬†<em>Jiaxin Zhang</em>,¬†and¬†Lifu Huang</div> <div class="periodical"> <em>In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</em>, 2025 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.23493" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2505.23493.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PLUM-Lab/R2I-Bench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><a href="https://r2i-bench.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Leaderboard</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reasoning</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Multimodality &amp; Vision</span><span style="display: inline-block; color: #dc3545; border: 1px solid #dc3545; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Outstanding Paper Nomination</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Reasoning is a fundamental capability often required in real-world text-to-image (T2I) generation, e.g., generating a bitten apple that has been left in the air for more than a week necessitates understanding temporal decay and commonsense concepts. While recent T2I models have made impressive progress in producing photorealistic images, their reasoning capability remains underdeveloped and insufficiently evaluated. To bridge this gap, we introduce LogoR2I-Bench, a comprehensive benchmark specifically designed to rigorously assess reasoning-driven T2I generation. LogoR2I-Bench comprises 3,068 meticulously curated data instances, spanning 7 core reasoning categories, including commonsense, mathematical, logical, compositional, numerical, causal, and concept mixing. To facilitate fine-grained evaluation, we design LogoR2I-Score, a QA-style metric based on instance-specific, reasoning-oriented evaluation questions that assess three critical dimensions: text-image alignment, reasoning accuracy, and image quality. Extensive experiments with 16 representative T2I models, including a strong pipeline-based framework that decouples reasoning and generation using the state-of-the-art language and image generation models, demonstrate consistently limited reasoning performance, highlighting the need for more robust, reasoning-aware architectures in the next generation of T2I systems.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP 2025</abbr></div> <div id="li2025statistical" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2502.20560" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Towards Statistical Factuality Guarantee For Large Vision-Language Models</a></div> <div class="author"> Zhuohang Li,¬†Chao Yan,¬†Nicholas J Jackson,¬†Wendi Cui,¬†Bo Li,¬†<em>Jiaxin Zhang</em>,¬†and¬†Bradley A Malin</div> <div class="periodical"> <em>In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</em>, 2025 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.20560" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2502.20560.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Multimodality &amp; Vision</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Advancements in Large Vision-Language Models (LVLMs) have demonstrated promising performance in a variety of vision-language tasks involving image-conditioned free-form text generation. However, growing concerns about hallucinations in LVLMs, where the generated text is inconsistent with the visual context, are becoming a major impediment to deploying these models in applications that demand guaranteed reliability. In this paper, we introduce a framework to address this challenge, ConfLVLM, which is grounded on conformal prediction to achieve finite-sample distribution-free statistical guarantees on the factuality of LVLM output. This framework treats an LVLM as a hypothesis generator, where each generated text detail (or claim) is considered an individual hypothesis. It then applies a statistical hypothesis testing procedure to verify each claim using efficient heuristic uncertainty measures to filter out unreliable claims before returning any responses to users. We conduct extensive experiments covering three representative application domains, including general scene understanding, medical radiology report generation, and document understanding. Remarkably, ConfLVLM reduces the error rate of claims generated by LLaVa-1.5 for scene descriptions from 87.8% to 10.0% by filtering out erroneous claims with a 95.3% true positive rate. Our results further demonstrate that ConfLVLM is highly flexible, and can be applied to any black-box LVLMs paired with any uncertainty measure for any image-conditioned free-form text generation task while providing a rigorous guarantee on controlling the risk of hallucination.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP 2025</abbr></div> <div id="zhang2025confidence" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2025.emnlp-industry.146.pdf" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Confidence-Aware Reasoning: Optimizing Self-Guided Thinking Trajectories in Large Reasoning Models</a></div> <div class="author"> <em>Jiaxin Zhang</em> </div> <div class="periodical"> <em>In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track</em>, 2025 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2025.emnlp-industry.146.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reasoning</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Chain-of-thought enables large reasoning models (LRMs) to reason through multi-step problems but often leads to unnecessarily long or redundant reasoning traces, a phenomenon known as overthinking. This results in inflated inference costs and potential degradation in answer quality. To address these challenges, we propose Confidence-Aware Reasoning (), an inference-time framework that optimizes reasoning trajectories by selectively pruning low-utility reasoning blocks and halting early when sufficient confidence has been achieved. is theoretically grounded in Bayesian optimal experimental design, treating each reasoning block as a sequential decision whose utility is approximated by its marginal contribution to reducing final answer uncertainty. We introduce a lightweight implementation that leverages token-level confidence to dynamically modulate reasoning depth without additional supervision. Evaluations on multiple benchmarks, including AMC, AIME, GPQA-Diamond, and MATH-500 show that improves answer accuracy by up to +13.3%, while reducing average reasoning length by 40%‚Äì50%. Our findings demonstrate that information-theoretic insights can effectively control self-guided reasoning and enable LRMs to "think just enough" at test time.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">ACL 2025</abbr></div> <div id="cui2025see" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2402.11347" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">SEE: Strategic Exploration and Exploitation for Cohesive In-Context Prompt Optimization</a></div> <div class="author"> Wendi Cui,¬†Zhuohang Li,¬†Hao Sun,¬†Damien Lopez,¬†Kamalika Das,¬†Bradley A Malin,¬†Sricharan Kumar,¬†and¬†<em>Jiaxin Zhang</em> </div> <div class="periodical"> <em>In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics</em>, 2025 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.11347" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2402.11347.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://www.linkedin.com/posts/shahules_ensuring-that-an-llm-judge-aligns-with-human-activity-7292247994014277632-RW5U/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Optimization</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Designing optimal prompts for Large Language Models (LLMs) is a complicated and resource-intensive task, often requiring substantial human expertise and effort. Existing approaches typically separate the optimization of prompt instructions and in-context learning examples, leading to incohesive prompts that are defined and represented by suboptimal task performance. To overcome these challenges, we propose a novel Cohesive In-Context Prompt Optimization framework that refines both prompt instructions and examples. However, formulating such an optimization in the discrete and high-dimensional space of natural language poses significant challenges in both convergence and computational efficiency. To address these issues, we introduce SEE, a scalable and efficient prompt optimization framework that adopts metaheuristic optimization principles and strategically balances exploration and exploitation to enhance optimization performance and achieve efficient convergence. SEE features a quad-phased design that alternates between global traversal (exploration) and local optimization (exploitation) and adaptively chooses LLM operators during the optimization process. We have conducted a comprehensive evaluation across 35 benchmark tasks, and SEE significantly outperforms state-of-the-art baseline methods by a large margin, achieving an average performance gain of 13.94 while reducing computational costs by 58.67%.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">ACL 2025</abbr></div> <div id="cui2025automatic" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2502.18746" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Automatic Prompt Optimization via Heuristic Search: A Survey</a></div> <div class="author"> Wendi Cui,¬†Zhuohang Li,¬†Hao Sun,¬†Damien Lopez,¬†Kamalika Das,¬†Bradley A Malin,¬†Sricharan Kumar,¬†and¬†<em>Jiaxin Zhang</em> </div> <div class="periodical"> <em>In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics</em>, 2025 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.18746" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2502.18746.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Optimization</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Recent advances in Large Language Models have led to remarkable achievements across a variety of Natural Language Processing tasks, making prompt engineering increasingly central to guiding model outputs. While manual methods can be effective, they typically rely on intuition and do not automatically refine prompts over time. In contrast, automatic prompt optimization employing heuristic-based search algorithms can systematically explore and improve prompts with minimal human oversight. This survey proposes a comprehensive taxonomy of these methods, categorizing them by where optimization occurs, what is optimized, what criteria drive the optimization, which operators generate new prompts, and which iterative search algorithms are applied. We further highlight specialized datasets and tools that support and accelerate automated prompt refinement. We conclude by discussing key open challenges pointing toward future opportunities for more robust and versatile LLM applications.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR 2025</abbr></div> <div id="xu2025modality" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2407.03604" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Modality-Specialized Synergizers for Interleaved Vision-Language Generalists</a></div> <div class="author"> Zhiyang Xu,¬†Minqian Liu,¬†Ying Shen,¬†Joy Rimchala,¬†<em>Jiaxin Zhang</em>,¬†Qifan Wang,¬†Yu Cheng,¬†and¬†Lifu Huang</div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, 2025 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2407.03604" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2407.03604.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Multimodality &amp; Vision</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Recent advancements in Vision-Language Models (VLMs) have led to the emergence of Vision-Language Generalists (VLGs) capable of understanding and generating both text and images. However, seamlessly generating an arbitrary sequence of text and images remains a challenging task for the current VLGs. One primary limitation lies in applying a unified architecture and the same set of parameters to simultaneously model discrete text tokens and continuous image features. Recent works attempt to tackle this fundamental problem by introducing modality-aware expert models. However, they employ identical architectures to process both text and images, disregarding the intrinsic inductive biases in these two modalities. In this work, we introduce MODALITY-SPECIALIZED SYNERGIZERS (MOSS), a novel design that efficiently optimizes existing unified architectures of VLGs with modality-specialized adaptation layers, i.e., a Convolutional LoRA for modeling the local priors of image patches and a Linear LoRA for processing sequential text. This design enables more effective modeling of modality-specific features while maintaining the strong cross-modal integration gained from pretraining. In addition, to improve the instruction-following capability on interleaved text-and-image generation, we introduce LEAFINSTRUCT, the first open-sourced interleaved instruction tuning dataset comprising 184,982 high-quality instances on more than 10 diverse domains. Extensive experiments show that VLGs integrated with M OSS achieve state-of-the-art performance, significantly surpassing baseline VLGs in complex interleaved generation tasks. Furthermore, our method exhibits strong generalizability on different VLGs.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">NAACL 2025</abbr></div> <div id="wang2025gradient" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2503.08963" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation</a></div> <div class="author"> Yu Wang,¬†Kamalika Das,¬†Xiang Gao,¬†Wendi Cui,¬†Peng Li,¬†and¬†<em>Jiaxin Zhang</em> </div> <div class="periodical"> <em>In Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics</em>, 2025 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2503.08963" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2503.08963.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Hallucination</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>In tasks like summarization and open-book question answering (QA), Large Language Models (LLMs) often encounter "contextual hallucination", where they produce irrelevant or incorrect responses despite having access to accurate source information. This typically occurs because these models tend to prioritize self-generated content over the input context, causing them to disregard pertinent details. To address this challenge, we introduce a novel method called "Guided Attention Map Editing" (GAME), which dynamically adjusts attention maps to improve contextual relevance. During inference, GAME employs a trained classifier to identify attention maps prone to inducing hallucinations and executes targeted interventions. These interventions, guided by gradient-informed "edit directions", strategically redistribute attention weights across various heads to effectively reduce hallucination. Comprehensive evaluations on challenging summarization and open-book QA tasks show that GAME consistently reduces hallucinations across a variety of open-source models. Specifically, GAME reduces hallucinations by 10% in the XSum summarization task while achieving a 7X speed-up in computational efficiency compared to the state-of-the-art baselines.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"> <abbr class="badge">EMNLP 2024</abbr><div style="margin-top: 0.25rem;"> <span style="display: inline-block; background-color: transparent; color: var(--global-theme-color); border: 1px solid var(--global-theme-color); padding: 0.15rem 0.5rem; border-radius: 8px; font-size: 0.7rem; font-weight: 500;">Oral</span> </div> </div> <div id="cui2024dcrfull" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2401.02132" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Divide-Conquer-Reasoning for Consistency Evaluation and Automatic Improvement of Large Language Models</a></div> <div class="author"> Wendi Cui,¬†Zhuohang Li,¬†Damien Lopez,¬†Kamalika Das,¬†Bradley Malin,¬†Sricharan Kumar,¬†and¬†<em>Jiaxin Zhang</em> </div> <div class="periodical"> <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track</em>, 2024 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2401.02132" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2401.02132.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/intuit-ai-research/DCR-consistency" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Hallucination</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Evaluating the quality and variability of text generated by Large Language Models (LLMs) poses a significant, yet unresolved research challenge. Traditional evaluation methods, such as ROUGE and BERTScore, which measure token similarity, often fail to capture the holistic semantic equivalence. This results in a low correlation with human judgments and intuition, which is especially problematic in high-stakes applications like healthcare and finance where reliability, safety, and robust decision-making are highly critical. This work proposes DCR, an automated framework for evaluating and improving the consistency of LLM-generated texts using a divide-conquer-reasoning approach. Unlike existing LLM-based evaluators that operate at the paragraph level, our method employs a divide-and-conquer evaluator (DCE) that breaks down the paragraph-to-paragraph comparison between two generated responses into individual sentence-to-paragraph comparisons, each evaluated based on predefined criteria. To facilitate this approach, we introduce an automatic metric converter (AMC) that translates the output from DCE into an interpretable numeric score. Beyond the consistency evaluation, we further present a reason-assisted improver (RAI) that leverages the analytical reasons with explanations identified by DCE to generate new responses aimed at reducing these inconsistencies. Through comprehensive and systematic empirical analysis, we show that our approach outperforms state-of-the-art methods by a large margin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the consistency of LLM generation across multiple benchmarks in semantic, factual, and summarization consistency tasks. Our approach also substantially reduces nearly 90% of output inconsistencies, showing promise for effective hallucination mitigation.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP 2024</abbr></div> <div id="zhang2024ski" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2410.09629" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models</a></div> <div class="author"> <em>Jiaxin Zhang</em>,¬†Wendi Cui,¬†Yiran Huang,¬†Kamalika Das,¬†and¬†Sricharan Kumar</div> <div class="periodical"> <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, 2024 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.09629" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2410.09629.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://medium.com/intuit-engineering/enhancing-llms-with-synthetic-knowledge-ingestion-a-novel-approach-from-intuit-ai-research-at-01e8f02b9c46" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a> <a href="https://github.com/intuit-ai-research/knowledge-infused-ai/tree/main/synthetic-knowledge-ingestion" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">RAG</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Large language models (LLMs) are proficient in capturing factual knowledge across various domains. However, refining their capabilities on previously seen knowledge or integrating new knowledge from external sources remains a significant challenge. In this work, we propose a novel synthetic knowledge ingestion method called Ski, which leverages fine-grained synthesis, interleaved generation, and assemble augmentation strategies to construct high-quality data representations from raw knowledge sources. We then integrate Ski and its variations with three knowledge injection techniques: Retrieval Augmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual Pre-training (CPT) to inject and refine knowledge in language models. Extensive empirical experiments are conducted on various question-answering tasks spanning finance, biomedicine, and open-generation domains to demonstrate that Ski significantly outperforms baseline methods by facilitating effective knowledge injection. We believe that our work is an important step towards enhancing the factual accuracy of LLM outputs by refining knowledge representation and injection capabilities.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"> <abbr class="badge">EMNLP 2024</abbr><div style="margin-top: 0.25rem;"> <span style="display: inline-block; background-color: transparent; color: var(--global-theme-color); border: 1px solid var(--global-theme-color); padding: 0.15rem 0.5rem; border-radius: 8px; font-size: 0.7rem; font-weight: 500;">Oral</span> </div> </div> <div id="li2024ook" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2410.08320" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation</a></div> <div class="author"> Zhuohang Li,¬†<em>Jiaxin Zhang</em>,¬†Chao Yan,¬†Kamalika Das,¬†Sricharan Kumar,¬†Murat Kantarcioglu,¬†and¬†Bradley A Malin</div> <div class="periodical"> <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, 2024 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.08320" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2410.08320" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">RAG</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Language models (LMs) are known to suffer from hallucinations and misinformation. Retrieval augmented generation (RAG) that retrieves verifiable information from an external knowledge corpus to complement the parametric knowledge in LMs provides a tangible solution to these problems. However, the generation quality of RAG is highly dependent on the relevance between a user‚Äôs query and the retrieved documents. Inaccurate responses may be generated when the query is outside of the scope of knowledge represented in the external knowledge corpus or if the information in the corpus is out-of-date. In this work, we establish a statistical framework that assesses how well a query can be answered by an RAG system by capturing the relevance of knowledge. We introduce an online testing procedure that employs goodness-of-fit (GoF) tests to inspect the relevance of each user query to detect out-of-knowledge queries with low knowledge relevance. Additionally, we develop an offline testing framework that examines a collection of user queries, aiming to detect significant shifts in the query distribution which indicates the knowledge corpus is no longer sufficiently capable of supporting the interests of the users. We demonstrate the capabilities of these strategies through a systematic evaluation on eight question-answering (QA) datasets, the results of which indicate that the new testing framework is an efficient solution to enhance the reliability of existing RAG systems.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP 2024</abbr></div> <div id="zhou2024hyqe" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2410.15262" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">HyQE: Ranking Contexts with Hypothetical Query Embeddings</a></div> <div class="author"> Weichao Zhou,¬†<em>Jiaxin Zhang</em>,¬†Hilaf Hasson,¬†Anu Singh,¬†and¬†Wenchao Li</div> <div class="periodical"> <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, 2024 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.15262" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2410.15262" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/zwc662/hyqe" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">RAG</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>In retrieval-augmented systems, context ranking techniques are commonly employed to reorder the retrieved contexts based on their relevance to a user query. A standard approach is to measure this relevance through the similarity between contexts and queries in the embedding space. However, such similarity often fails to capture the relevance. Alternatively, large language models (LLMs) have been used for ranking contexts. However, they can encounter scalability issues when the number of candidate contexts grows and the context window sizes of the LLMs remain constrained. Additionally, these approaches require fine-tuning LLMs with domain-specific data. In this work, we introduce a scalable ranking framework that combines embedding similarity and LLM capabilities without requiring LLM fine-tuning. Our framework uses a pre-trained LLM to hypothesize the user query based on the retrieved contexts and ranks the context based on the similarity between the hypothesized queries and the user query. Our framework is efficient at inference time and is compatible with many other retrieval and ranking techniques. Experimental results show that our method improves the ranking performance across multiple benchmarks.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP 2024</abbr></div> <div id="liu2024holis" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2406.14643" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Holistic evaluation for interleaved text-and-image generation</a></div> <div class="author"> Minqian Liu,¬†Zhiyang Xu,¬†Zihao Lin,¬†Trevor Ashby,¬†Joy Rimchala,¬†<em>Jiaxin Zhang</em>,¬†and¬†Lifu Huang</div> <div class="periodical"> <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, 2024 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.14643" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2406.14643" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://huggingface.co/mqliu/InterleavedBench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://github.com/VT-NLP/InterleavedBench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><a href="https://vt-nlp.github.io/InterleavedEval/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Multimodality &amp; Vision</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Interleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate and explainable evaluation. We carefully define five essential evaluation aspects for InterleavedEval, including text quality, perceptual quality, image coherence, text-image coherence, and helpfulness, to ensure a comprehensive and fine-grained assessment. Through extensive experiments and rigorous human evaluation, we show that our benchmark and metric can effectively evaluate the existing models with a strong correlation with human judgments surpassing previous reference-based metrics. We also provide substantial findings and insights to foster future research in interleaved generation and its evaluation.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP 2024</abbr></div> <div id="sinha2024sos" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2410.09652" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Survival of the Safest: Towards Secure Prompt Optimization through Interleaved Multi-Objective Evolution</a></div> <div class="author"> Ankita Sinha,¬†Wendi Cui,¬†Kamalika Das,¬†and¬†<em>Jiaxin Zhang</em> </div> <div class="periodical"> <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing - Industry Track</em>, 2024 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.09652" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2410.09652" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Optimization</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have demonstrated remarkable capabilities; however, the optimization of their prompts has historically prioritized performance metrics at the expense of crucial safety and security considerations. To overcome this shortcoming, we introduce "Survival of the Safest" (SoS), an innovative multi-objective prompt optimization framework that enhances both performance and security in LLMs simultaneously. SoS utilizes an interleaved multi-objective evolution strategy, integrating semantic, feedback, and crossover mutations to effectively traverse the prompt landscape. Differing from the computationally demanding Pareto front methods, SoS provides a scalable solution that expedites optimization in complex, high-dimensional discrete search spaces while keeping computational demands low. Our approach accommodates flexible weighting of objectives and generates a pool of optimized candidates, empowering users to select prompts that optimally meet their specific performance and security needs. Experimental evaluations across diverse benchmark datasets affirm SoS‚Äôs efficacy in delivering high performance and notably enhancing safety and security compared to single-objective methods. This advancement marks a significant stride towards the deployment of LLM systems that are both high-performing and secure across varied industrial applications</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">EACL 2024</abbr></div> <div id="gao2024spuq" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2403.02509" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models</a></div> <div class="author"> Xiang Gao,¬†<em>Jiaxin Zhang</em>,¬†Lalla Mouatadid,¬†and¬†Kamalika Das</div> <div class="periodical"> <em>In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics</em>, 2024 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.02509" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2403.02509" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://medium.com/intuit-engineering/intuit-presents-innovative-approach-to-quantifying-llm-uncertainty-at-eacl-2024-f839a8f1b89b" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a> <a href="https://github.com/intuit-ai-research/SPUQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggregation techniques. Our findings show a substantial improvement in model uncertainty calibration, with a reduction in Expected Calibration Error (ECE) by 50% on average. Our findings suggest that our proposed UQ method offers promising steps toward enhancing the reliability and trustworthiness of LLMs.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">AISTATS 2024</abbr></div> <div id="zhang2024discriminant" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2402.12664" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Discriminant Distance-Aware Representation on Deterministic Uncertainty Quantification Methods</a></div> <div class="author"> <em>Jiaxin Zhang</em>,¬†Kamalika Das,¬†and¬†Sricharan Kumar</div> <div class="periodical"> <em>In International Conference on Artificial Intelligence and Statistics</em>, 2024 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.12664" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2402.12664" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">UQ</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Uncertainty estimation is a crucial aspect of deploying dependable deep learning models in safety-critical systems. In this study, we introduce a novel and efficient method for deterministic uncertainty estimation called Discriminant Distance-Awareness Representation (DDAR). Our approach involves constructing a DNN model that incorporates a set of prototypes in its latent representations, enabling us to analyze valuable feature information from the input data. By leveraging a distinction maximization layer over optimal trainable prototypes, DDAR can learn a discriminant distance-awareness representation. We demonstrate that DDAR overcomes feature collapse by relaxing the Lipschitz constraint that hinders the practicality of deterministic uncertainty methods (DUMs) architectures. Our experiments show that DDAR is a flexible and architecture-agnostic method that can be easily integrated as a pluggable layer with distance-sensitive metrics, outperforming state-of-the-art uncertainty estimation methods on multiple benchmark problems.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">WACV 2024</abbr></div> <div id="zhang2023robustness" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2311.09625" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">DECDM: Document Enhancement using Cycle-Consistent Diffusion Models</a></div> <div class="author"> <em>Jiaxin Zhang</em>,¬†Joy Rimchala,¬†Lalla Mouatadid,¬†Kamalika Das,¬†and¬†Sricharan Kumar</div> <div class="periodical"> <em>In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2024 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.09625" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2311.09625" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://medium.com/intuit-engineering/wacv-2024-intuit-ai-research-develops-end-to-end-method-for-document-enhancement-using-diffusion-a4e4a77b8e40" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Multimodality &amp; Vision</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>The performance of optical character recognition (OCR) heavily relies on document image quality, which is crucial for automatic document processing and document intelligence. However, most existing document enhancement methods require supervised data pairs, which raises concerns about data separation and privacy protection, and makes it challenging to adapt these methods to new domain pairs. To address these issues, we propose DECDM, an end-to-end document-level image translation method inspired by recent advances in diffusion models. Our method overcomes the limitations of paired training by independently training the source (noisy input) and target (clean output) models, making it possible to apply domain-specific diffusion models to other pairs. DECDM trains on one dataset at a time, eliminating the need to scan both datasets concurrently, and effectively preserving data privacy from the source or target domain. We also introduce simple data augmentation strategies to improve character-glyph conservation during translation. We compare DECDM with state-of-the-art methods on multiple synthetic data and benchmark datasets, such as document denoising and shadow removal, and demonstrate the superiority of performance quantitatively and qualitatively.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">WACV 2024</abbr></div> <div id="zhang2023robustnest" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2311.09639" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">On the Quantification of Image Reconstruction Uncertainty without Training Data</a></div> <div class="author"> <em>Jiaxin Zhang</em>,¬†Sirui Bi,¬†and¬†Victor Fung</div> <div class="periodical"> <em>In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2024 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.09639" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2311.09639" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">UQ</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Multimodality &amp; Vision</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Computational imaging plays a pivotal role in determining hidden information from sparse measurements. A robust inverse solver is crucial to fully characterize the uncertainty induced by these measurements, as it allows for the estimation of the complete posterior of unrecoverable targets. This, in turn, facilitates a probabilistic interpretation of observational data for decision-making. In this study, we propose a deep variational framework that leverages a deep generative model to learn an approximate posterior distribution to effectively quantify image reconstruction uncertainty without the need for training data. We parameterize the target posterior using a flow-based model and minimize their Kullback-Leibler (KL) divergence to achieve accurate uncertainty estimation. To bolster stability, we introduce a robust flow-based model with bi-directional regularization and enhance expressivity through gradient boosting. Additionally, we incorporate a space-filling design to achieve substantial variance reduction on both latent prior space and target posterior space. We validate our method on several benchmark tasks and two real-world applications, namely fastMRI and black hole image reconstruction. Our results indicate that our method provides reliable and high-quality image reconstruction with robust uncertainty estimation.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS 2023</abbr></div> <div id="zhang2023interactive" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2310.20153" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision</a></div> <div class="author"> <em>Jiaxin Zhang</em>,¬†Zhuohang Li,¬†Kamalika Das,¬†and¬†Sricharan Kumar</div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.20153" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2310.20153" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://medium.com/intuit-engineering/neurips-2023-intuit-ai-research-presents-interactive-framework-for-cost-effective-fine-tuning-of-8c1f6af5b87a" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a> <a href="/assets/pdf/publications/NeurIPS2023_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/publications/Neurips2023_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation, and 2) variable batch size that controls the order for choosing each fidelity to facilitate knowledge distillation, ultimately enhancing annotation quality. Extensive experiments on financial and medical tasks demonstrate that IMFL achieves superior performance compared with single fidelity annotations. Given a limited budget of human annotation, IMFL significantly outperforms the human annotation baselines in all four tasks and achieves very close performance as human annotations on two of the tasks. These promising results suggest that the high human annotation costs in domain-specific tasks can be significantly reduced by employing IMFL, which utilizes fewer human annotations, supplemented with cheaper and faster LLM (e.g., GPT-3.5) annotations to achieve comparable performance.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP 2023</abbr></div> <div id="zhang2023sac3" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2311.01740" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">SAC^3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency</a></div> <div class="author"> <em>Jiaxin Zhang</em>,¬†Zhuohang Li,¬†Kamalika Das,¬†Bradley Malin,¬†and¬†Sricharan Kumar</div> <div class="periodical"> <em>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, 2023 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.01740" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2311.01740" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><a href="https://medium.com/intuit-engineering/intuit-ai-research-debuts-novel-approach-to-reliable-hallucination-detection-in-black-box-language-746d7f720c50" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a> <a href="https://github.com/intuit/sac3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Hallucination</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">AAAI 2023</abbr></div> <div id="zhang2022accelerating" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2212.01016" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Accelerating Inverse Learning via Intelligent Localization with Exploratory Sampling</a></div> <div class="author"> <em>Jiaxin Zhang</em>,¬†Sirui Bi,¬†and¬†Victor Fung</div> <div class="periodical"> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2023 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2212.01016" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2212.01016.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/jxzhangjhu/MatDesINNe" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Optimization</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">AI4Science</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>In the scope of "AI for Science", solving inverse problems is a longstanding challenge in materials and drug discovery, where the goal is to determine the hidden structures given a set of desirable properties. Deep generative models are recently proposed to solve inverse problems, but these currently use expensive forward operators and struggle in precisely localizing the exact solutions and fully exploring the parameter spaces without missing solutions. In this work, we propose a novel approach (called iPage) to accelerate the inverse learning process by leveraging probabilistic inference from deep invertible models and deterministic optimization via fast gradient descent. Given a target property, the learned invertible model provides a posterior over the parameter space; we identify these posterior samples as an intelligent prior initialization which enables us to narrow down the search space. We then perform gradient descent to calibrate the inverse solutions within a local region. Meanwhile, a space-filling sampling is imposed on the latent space to better explore and capture all possible solutions. We evaluate our approach on three benchmark tasks and two created datasets with real-world applications from quantum chemistry and additive manufacturing, and find our method achieves superior performance compared to several state-of-the-art baseline methods.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"> <abbr class="badge">AAAI 2023</abbr><div style="margin-top: 0.25rem;"> <span style="display: inline-block; background-color: transparent; color: var(--global-theme-color); border: 1px solid var(--global-theme-color); padding: 0.15rem 0.5rem; border-radius: 8px; font-size: 0.7rem; font-weight: 500;">Oral</span> </div> </div> <div id="wangautonf" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2212.01016" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">AutoNF: Automated Architecture Optimization of Normalizing Flows Using a Mixture Distribution Formulation</a></div> <div class="author"> Yu Wang,¬†Jan Drgona,¬†<em>Jiaxin Zhang</em>,¬†Karthik Somayaji NS,¬†Frank Y Liu,¬†Malachi Schram,¬†and¬†Peng Li</div> <div class="periodical"> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2023 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2212.01016" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2304.05135.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Optimization</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Normalizing flows (NF) build upon invertible neural networks and have wide applications in probabilistic modeling. Currently, building a powerful yet computationally efficient flow model relies on empirical fine-tuning over a large design space. While introducing neural architecture search (NAS) to NF is desirable, the invertibility constraint of NF brings new challenges to existing NAS methods whose application is limited to unstructured neural networks. Developing efficient NAS methods specifically for NF remains an open problem. We present AutoNF, the first automated NF architectural optimization framework. First, we present a new mixture distribution formulation that allows efficient differentiable architecture search of flow models without violating the invertibility constraint. Second, under the new formulation, we convert the original NP-hard combinatorial NF architectural optimization problem to an unconstrained continuous relaxation admitting the discrete optimal architectural solution, circumventing the loss of optimality due to binarization in architectural optimization. We evaluate AutoNF with various density estimation datasets and show its superior performance-cost trade-offs over a set of existing hand-crafted baselines.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">CVPR 2022</abbr></div> <div id="li2022auditing" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2203.15696" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage</a></div> <div class="author"> Zhuohang Li,¬†<em>Jiaxin Zhang</em>,¬†Luyang Liu,¬†and¬†Jian Liu</div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2203.15696" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Auditing_Privacy_Defenses_in_Federated_Learning_via_Generative_Gradient_Leakage_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/zhuohangli/GGL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Optimization</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Multimodality &amp; Vision</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Federated Learning (FL) framework brings privacy benefits to distributed learning systems by allowing multiple clients to participate in a learning task under the coordination of a central server without exchanging their private data. However, recent studies have revealed that private information can still be leaked through shared gradient information. To further protect user‚Äôs privacy, several defense mechanisms have been proposed to prevent privacy leakage via gradient information degradation methods, such as using additive noise or gradient compression before sharing it with the server. In this work, we validate that the private training data can still be leaked under certain defense settings with a new type of leakage, ie, Generative Gradient Leakage (GGL). Unlike existing methods that only rely on gradient information to reconstruct data, our method leverages the latent space of generative adversarial networks (GAN) learned from public image datasets as a prior to compensate for the informational loss during gradient degradation. To address the nonlinearity caused by the gradient operator and the GAN model, we explore various gradient-free optimization methods (eg, evolution strategies and Bayesian optimization) and empirically show their superiority in reconstructing high-quality images from gradients compared to gradient-based optimizers. We hope the proposed method can serve as a tool for empirically measuring the amount of privacy leakage to facilitate the design of more robust defense mechanisms.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">AAAI 2022</abbr></div> <div id="sun2022gradient" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2112.09815" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Gradient-based Novelty Detection Boosted by Self-supervised Binary Classification</a></div> <div class="author"> Jingbo Sun,¬†Li Yang,¬†<em>Jiaxin Zhang</em>,¬†Frank Liu,¬†Mahantesh Halappanavar,¬†Deliang Fan,¬†and¬†Yu Cao</div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2022 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2112.09815" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20812" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Robustness</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Multimodality &amp; Vision</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Novelty detection aims to automatically identify out-of-distribution (OOD) data, without any prior knowledge of them. It is a critical step in data monitoring, behavior analysis and other applications, helping enable continual learning in the field. Conventional methods of OOD detection perform multi-variate analysis on an ensemble of data or features, and usually resort to the supervision with OOD data to improve the accuracy. In reality, such supervision is impractical as one cannot anticipate the anomalous data. In this paper, we propose a novel, self-supervised approach that does not rely on any pre-defined OOD data:(1) The new method evaluates the Mahalanobis distance of the gradients between the in-distribution and OOD data.(2) It is assisted by a self-supervised binary classifier to guide the label selection to generate the gradients, and maximize the Mahalanobis distance. In the evaluation with multiple datasets, such as CIFAR-10, CIFAR-100, SVHN and TinyImageNet, the proposed approach consistently outperforms state-of-the-art supervised and unsupervised methods in the area under the receiver operating characteristic (AUROC) and area under the precision-recall curve (AUPR) metrics. We further demonstrate that this detector is able to accurately learn one OOD class in continual learning.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS 2021</abbr></div> <div id="drgona2021stochastic" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2111.04601" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">On the Stochastic Stability of Deep Markov Models</a></div> <div class="author"> Jan Drgona,¬†Sayak Mukherjee,¬†<em>Jiaxin Zhang</em>,¬†Frank Liu,¬†and¬†Mahantesh Halappanavar</div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2021 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2111.04601" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.neurips.cc/paper/2021/file/c9dd73f5cb96486f5e1e0680e841a550-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/pnnl/slim" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Reliability &amp; Safety</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Deep Markov models (DMM) are generative models which are scalable and expressive generalization of Markov models for representation, learning, and inference problems. However, the fundamental stochastic stability guarantees of such models have not been thoroughly investigated. In this paper, we present a novel stability analysis method and provide sufficient conditions of DMM‚Äôs stochastic stability. The proposed stability analysis is based on the contraction of probabilistic maps modeled by deep neural networks. We make connections between the spectral properties of neural network‚Äôs weights and different types of used activation function on the stability and overall dynamic behavior of DMMs with Gaussian distributions. Based on the theory, we propose a few practical methods for designing constrained DMMs with guaranteed stability. We empirically substantiate our theoretical results via intuitive numerical experiments using the proposed stability constraints.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">UAI 2021</abbr></div> <div id="zhang2021enabling" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2002.03001" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Enabling Long-range Exploration in Minimization of Multimodal Functions</a></div> <div class="author"> <em>Jiaxin Zhang</em>,¬†Hoang Tran,¬†Dan Lu,¬†and¬†Guannan Zhang</div> <div class="periodical"> <em>In Uncertainty in Artificial Intelligence</em>, 2021 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2002.03001" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.mlr.press/v161/zhang21e/zhang21e.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/HoangATran/AdaDGS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Optimization</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>We consider the problem of minimizing multi-modal loss functions with a large number of local optima. Since the local gradient points to the direction of the steepest slope in an infinitesimal neighborhood, an optimizer guided by the local gradient is often trapped in a local minimum. To address this issue, we develop a novel nonlocal gradient to skip small local minima by capturing major structures of the loss‚Äôs landscape in black-box optimization. The nonlocal gradient is defined by a directional Gaussian smoothing (DGS) approach. The key idea of DGS is to conducts 1D long-range exploration with a large smoothing radius along orthogonal directions in , each of which defines a nonlocal directional derivative as a 1D integral. Such long-range exploration enables the nonlocal gradient to skip small local minima. The directional derivatives are then assembled to form the nonlocal gradient. We use the Gauss-Hermite quadrature rule to approximate the 1D integrals to obtain an accurate estimator. The superior performance of our method is demonstrated in three sets of examples, including benchmark functions for global optimization, and two real-world scientific problems.</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">AISTATS 2021</abbr></div> <div id="zhang2021scalable" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/2103.08026" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">A Scalable Gradient Free Method for Bayesian Experimental Design with Implicit Models</a></div> <div class="author"> <em>Jiaxin Zhang</em>,¬†Sirui Bi,¬†and¬†Guannan Zhang</div> <div class="periodical"> <em>In International Conference on Artificial Intelligence and Statistics</em>, 2021 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2103.08026" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="http://proceedings.mlr.press/v130/zhang21l/zhang21l.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Optimization</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">AI4Science</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Bayesian experimental design (BED) is to answer the question that how to choose designs that maximize the information gathering. For implicit models, where the likelihood is intractable but sampling is possible, conventional BED methods have difficulties in efficiently estimating the posterior distribution and maximizing the mutual information (MI) between data and parameters. Recent work proposed the use of gradient ascent to maximize a lower bound on MI to deal with these issues. However, the approach requires a sampling path to compute the pathwise gradient of the MI lower bound with respect to the design variables, and such a pathwise gradient is usually inaccessible for implicit models. In this paper, we propose a novel approach that leverages recent advances in stochastic approximate gradient ascent incorporated with a smoothed variational MI estimator for efficient and robust BED. Without the necessity of pathwise gradients, our approach allows the design process to be achieved through a unified procedure with an approximate gradient for implicit models. Several experiments show that our approach outperforms baseline methods, and significantly improves the scalability of BED in high-dimensional problems</p> </div> </div> </div> </li> <li> <div class="row" style="width: 100%;"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS 2019</abbr></div> <div id="zhang2019learning" class="col-sm-10"> <div class="title"><a href="http://arxiv.org/abs/1902.10652" style="color: #2698BA; text-decoration: none;" rel="external nofollow noopener" target="_blank">Learning Nonlinear Level Sets for Dimensionality Reduction in Function Approximation</a></div> <div class="author"> Guannan Zhang,¬†<em>Jiaxin Zhang</em>,¬†and¬†Jacob Hinkle</div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em>, 2019 </div> <div class="periodical"> </div> <div class="links" style="margin-top: 0.5rem;"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1902.10652" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.neurips.cc/paper/2019/file/464074179972cbbd75a39abc6954cd12-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://mathlab.github.io/ATHENA/tutorial7nll.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">Optimization</span><span style="display: inline-block; color: #ff8c00; border: 1px solid #ff8c00; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 500; background-color: transparent; margin-left: 0.5rem; margin-right: 0.5rem;">AI4Science</span> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>We developed a Nonlinear Level-set Learning (NLL) method for dimensionality reduction in high-dimensional function approximation with small data. This work is motivated by a variety of design tasks in real-world engineering applications, where practitioners would replace their computationally intensive physical models (e.g., high-resolution fluid simulators) with fast-to-evaluate predictive machine learning models, so as to accelerate the engineering design processes. There are two major challenges in constructing such predictive models: (a) high-dimensional inputs (e.g., many independent design parameters) and (b) small training data, generated by running extremely time-consuming simulations. Thus, reducing the input dimension is critical to alleviate the over-fitting issue caused by data insufficiency. Existing methods, including sliced inverse regression and active subspace approaches, reduce the input dimension by learning a linear coordinate transformation; our main contribution is to extend the transformation approach to a nonlinear regime. Specifically, we exploit reversible networks (RevNets) to learn nonlinear level sets of a high-dimensional function and parameterize its level sets in low-dimensional spaces. A new loss function was designed to utilize samples of the target functions‚Äô gradient to encourage the transformed function to be sensitive to only a few transformed coordinates. The NLL approach is demonstrated by applying it to three 2D functions and two 20D functions for showing the improved approximation accuracy with the use of nonlinear transformation, as well as to an 8D composite material design problem for optimizing the buckling-resistance performance of composite shells of rocket inter-stages. </p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2026 Jiaxin Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: February 10, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>